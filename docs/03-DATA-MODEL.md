# Data Model Document: Foundry

Generated by: Data Modeling Agent v18 (Application-Agnostic)
Date: 2026-01-22
Status: Draft

Framework: Agent Specification Framework v2.1
Constitution: Agent 0 - Agent Constitution v3.3

---

## 1. Entity Overview

### CHECKPOINT 1: Entity Analysis

Based on PRD user stories and Architecture specifications, the following entities are required:

### Entities

| Entity | Purpose | Key Relationships |
|--------|---------|-------------------|
| organizations | Multi-tenant workspace containers | Has many users, projects, invitations |
| users | System users with authentication | Belongs to organization, has many projects |
| invitations | Pending team member invitations | Belongs to organization, created by user |
| password_resets | Temporary password reset tokens | Belongs to user |
| projects | Data preparation work containers | Belongs to organization, has many sources, runs |
| sources | Data inputs (files or API connections) | Belongs to project, has one schema_mapping, deidentification_config |
| schema_mappings | Field mapping configurations | Belongs to source |
| deidentification_configs | PII detection and masking rules | Belongs to source |
| processing_configs | Pipeline stage configurations | Belongs to project |
| processing_runs | Execution history of processing | Belongs to project, produces one dataset |
| datasets | Processed output data | Belongs to processing_run |
| audit_logs | PII handling audit trail | References multiple entities |

### Entity Relationship Diagram

```
organizations 1──< users
      │
      ├──< invitations
      │
      └──< projects 1──< sources 1──1 schema_mappings
                │              │
                │              └──1 deidentification_configs
                │
                ├──1 processing_configs
                │
                └──< processing_runs 1──1 datasets

users 1──< password_resets
  │
  └──< audit_logs
```

### Key Design Decisions

1. **Single Organization per User**: Per PRD AR-001 assumption, users belong to exactly one organization
2. **Soft Delete for Projects**: Projects use `deletedAt` pattern per PRD US-PROJ-004
3. **Hard Delete for Sources**: Cascade delete when project deleted per PRD data lifecycle
4. **Encrypted API Keys**: Teamwork API keys stored encrypted (AES-256-GCM) per Architecture Section 7
5. **JSONB for Flexible Config**: Schema mappings, PII rules, and processing results use JSONB for flexibility

---

## 2. Schema Definition

### CHECKPOINT 2: Complete Schema

```typescript
// server/db/schema.ts
import { 
  pgTable, 
  serial, 
  text, 
  timestamp, 
  integer, 
  boolean,
  jsonb,
  index,
  uniqueIndex
} from 'drizzle-orm/pg-core';

// ============================================================================
// AUDIT COLUMNS (Applied to all tables)
// ============================================================================

const auditColumns = {
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
};

// ============================================================================
// ORGANIZATIONS
// ============================================================================

export const organizations = pgTable('organizations', {
  id: serial('id').primaryKey(),
  name: text('name').notNull(),
  ...auditColumns,
});

// ============================================================================
// USERS
// ============================================================================

export const users = pgTable('users', {
  id: serial('id').primaryKey(),
  organizationId: integer('organization_id')
    .notNull()
    .references(() => organizations.id, { onDelete: 'cascade' }),
  email: text('email').notNull().unique(),
  passwordHash: text('password_hash').notNull(),
  name: text('name'),
  role: text('role').notNull().default('member'), // 'admin' | 'member'
  ...auditColumns,
}, (table) => ({
  organizationIdx: index('users_organization_idx').on(table.organizationId),
  emailIdx: uniqueIndex('users_email_idx').on(table.email),
}));

// ============================================================================
// INVITATIONS
// ============================================================================

export const invitations = pgTable('invitations', {
  id: serial('id').primaryKey(),
  organizationId: integer('organization_id')
    .notNull()
    .references(() => organizations.id, { onDelete: 'cascade' }),
  invitedById: integer('invited_by_id')
    .notNull()
    .references(() => users.id, { onDelete: 'cascade' }),
  email: text('email').notNull(),
  token: text('token').notNull().unique(),
  role: text('role').notNull().default('member'), // Role to assign on registration
  expiresAt: timestamp('expires_at').notNull(),
  usedAt: timestamp('used_at'), // null = not yet used
  ...auditColumns,
}, (table) => ({
  organizationIdx: index('invitations_organization_idx').on(table.organizationId),
  tokenIdx: uniqueIndex('invitations_token_idx').on(table.token),
  emailOrgIdx: index('invitations_email_org_idx').on(table.email, table.organizationId),
}));

// ============================================================================
// PASSWORD RESETS
// ============================================================================

export const passwordResets = pgTable('password_resets', {
  id: serial('id').primaryKey(),
  userId: integer('user_id')
    .notNull()
    .references(() => users.id, { onDelete: 'cascade' }),
  tokenHash: text('token_hash').notNull(), // Store hash, not plaintext
  expiresAt: timestamp('expires_at').notNull(),
  usedAt: timestamp('used_at'), // null = not yet used
  createdAt: timestamp('created_at').defaultNow().notNull(),
}, (table) => ({
  userIdx: index('password_resets_user_idx').on(table.userId),
}));

// ============================================================================
// PROJECTS
// ============================================================================

export const projects = pgTable('projects', {
  id: serial('id').primaryKey(),
  organizationId: integer('organization_id')
    .notNull()
    .references(() => organizations.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  description: text('description'),
  deletedAt: timestamp('deleted_at'), // Soft delete: null = active
  ...auditColumns,
}, (table) => ({
  organizationIdx: index('projects_organization_idx').on(table.organizationId),
  nameOrgIdx: uniqueIndex('projects_name_org_idx')
    .on(table.organizationId, table.name)
    .where(sql`deleted_at IS NULL`), // Unique name within org for active projects
}));

// ============================================================================
// SOURCES
// ============================================================================

export const sources = pgTable('sources', {
  id: serial('id').primaryKey(),
  projectId: integer('project_id')
    .notNull()
    .references(() => projects.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  type: text('type').notNull(), // 'file' | 'teamwork_desk'
  status: text('status').notNull().default('pending'), // 'pending' | 'connected' | 'syncing' | 'ready' | 'error'
  
  // File source fields
  fileName: text('file_name'),
  filePath: text('file_path'), // Local filesystem path
  fileSize: integer('file_size'), // Bytes
  fileType: text('file_type'), // 'csv' | 'xlsx' | 'json'
  
  // API source fields (Teamwork Desk)
  apiKey: text('api_key'), // ENCRYPTED: iv:encrypted:authTag format
  apiSubdomain: text('api_subdomain'), // {subdomain}.teamwork.com
  dataTypes: jsonb('data_types'), // ['tickets', 'conversations']
  
  // Cached data
  cachedData: jsonb('cached_data'), // Parsed/synced source data
  cachedAt: timestamp('cached_at'),
  cacheExpiresAt: timestamp('cache_expires_at'), // 30-day TTL
  
  // Detected schema (auto-detection results)
  detectedFields: jsonb('detected_fields'), // Array of { name, type, samples }
  
  lastSyncAt: timestamp('last_sync_at'),
  lastError: text('last_error'),
  ...auditColumns,
}, (table) => ({
  projectIdx: index('sources_project_idx').on(table.projectId),
  statusIdx: index('sources_status_idx').on(table.status),
}));

// ============================================================================
// SCHEMA MAPPINGS
// ============================================================================

export const schemaMappings = pgTable('schema_mappings', {
  id: serial('id').primaryKey(),
  sourceId: integer('source_id')
    .notNull()
    .unique()
    .references(() => sources.id, { onDelete: 'cascade' }),
  
  // Mapping configuration
  mappings: jsonb('mappings').notNull().default('[]'),
  // Structure: Array<{
  //   sourceField: string,
  //   targetField: string | null (null = skip),
  //   confidence: 'high' | 'medium' | 'low' | 'manual',
  //   transform?: string
  // }>
  
  // Canonical schema target
  targetSchema: text('target_schema').notNull().default('conversation'),
  // 'conversation' | 'qa_pairs' | 'raw'
  
  isConfigured: boolean('is_configured').notNull().default(false),
  ...auditColumns,
}, (table) => ({
  sourceIdx: uniqueIndex('schema_mappings_source_idx').on(table.sourceId),
}));

// ============================================================================
// DEIDENTIFICATION CONFIGS
// ============================================================================

export const deidentificationConfigs = pgTable('deidentification_configs', {
  id: serial('id').primaryKey(),
  sourceId: integer('source_id')
    .notNull()
    .unique()
    .references(() => sources.id, { onDelete: 'cascade' }),
  
  // PII type toggles
  enabledTypes: jsonb('enabled_types').notNull().default('[]'),
  // Array of: 'email' | 'phone' | 'name' | 'address' | 'ssn' | 'credit_card' | 'dob'
  
  // Custom patterns
  customPatterns: jsonb('custom_patterns').notNull().default('[]'),
  // Array<{ name: string, pattern: string (regex), replacement: string }>
  
  // Masking strategy
  maskingStrategy: text('masking_strategy').notNull().default('replacement'),
  // 'replacement' ([EMAIL], [PHONE]) | 'redaction' ([REDACTED]) | 'pseudonymization' (fake values)
  
  isConfigured: boolean('is_configured').notNull().default(false),
  ...auditColumns,
}, (table) => ({
  sourceIdx: uniqueIndex('deidentification_configs_source_idx').on(table.sourceId),
}));

// ============================================================================
// PROCESSING CONFIGS
// ============================================================================

export const processingConfigs = pgTable('processing_configs', {
  id: serial('id').primaryKey(),
  projectId: integer('project_id')
    .notNull()
    .unique()
    .references(() => projects.id, { onDelete: 'cascade' }),
  
  // Pipeline stages configuration
  stages: jsonb('stages').notNull().default('[]'),
  // Array<{
  //   name: 'ingest' | 'normalize' | 'deidentify' | 'filter' | 'role_identify' | 'export',
  //   enabled: boolean,
  //   config: object
  // }>
  
  // Quality filters
  filters: jsonb('filters').notNull().default('{}'),
  // {
  //   minMessageLength?: number,
  //   requireCompleteConversation?: boolean,
  //   excludePatterns?: string[]
  // }
  
  // Role identification
  roleIdentification: jsonb('role_identification').notNull().default('{}'),
  // {
  //   enabled: boolean,
  //   agentPatterns?: string[],
  //   customerPatterns?: string[],
  //   systemPatterns?: string[]
  // }
  
  // Export format
  exportFormat: text('export_format').notNull().default('jsonl'),
  // 'jsonl' | 'json' | 'csv' | 'qa_pairs'
  
  isConfigured: boolean('is_configured').notNull().default(false),
  ...auditColumns,
}, (table) => ({
  projectIdx: uniqueIndex('processing_configs_project_idx').on(table.projectId),
}));

// ============================================================================
// PROCESSING RUNS (Job Queue)
// ============================================================================

export const processingRuns = pgTable('processing_runs', {
  id: serial('id').primaryKey(),
  projectId: integer('project_id')
    .notNull()
    .references(() => projects.id, { onDelete: 'cascade' }),
  
  // Job status (per Architecture ADR-003)
  status: text('status').notNull().default('pending'),
  // 'pending' | 'processing' | 'completed' | 'cancelled' | 'failed'
  
  // Progress tracking
  progress: integer('progress').notNull().default(0), // 0-100
  currentStage: text('current_stage'), // Current pipeline stage name
  
  // Configuration snapshot (preserved for audit)
  configSnapshot: jsonb('config_snapshot').notNull(),
  
  // Results
  inputRecordCount: integer('input_record_count'),
  outputRecordCount: integer('output_record_count'),
  filteredRecordCount: integer('filtered_record_count'),
  
  // Timing
  startedAt: timestamp('started_at'),
  completedAt: timestamp('completed_at'),
  
  // Error handling
  error: text('error'),
  errorDetails: jsonb('error_details'),
  
  ...auditColumns,
}, (table) => ({
  projectIdx: index('processing_runs_project_idx').on(table.projectId),
  statusIdx: index('processing_runs_status_idx').on(table.status),
  createdAtIdx: index('processing_runs_created_at_idx').on(table.createdAt),
}));

// ============================================================================
// DATASETS
// ============================================================================

export const datasets = pgTable('datasets', {
  id: serial('id').primaryKey(),
  processingRunId: integer('processing_run_id')
    .notNull()
    .unique()
    .references(() => processingRuns.id, { onDelete: 'cascade' }),
  projectId: integer('project_id')
    .notNull()
    .references(() => projects.id, { onDelete: 'cascade' }),
  
  // Dataset metadata
  name: text('name').notNull(),
  format: text('format').notNull(), // 'jsonl' | 'json' | 'csv' | 'qa_pairs'
  recordCount: integer('record_count').notNull(),
  
  // Data storage (JSONB for flexibility, can migrate to file for large datasets)
  data: jsonb('data').notNull(),
  
  // Export info
  filePath: text('file_path'), // Generated export file path
  fileSize: integer('file_size'), // Bytes
  
  // Filtered records sample (for review)
  filteredSample: jsonb('filtered_sample'), // Sample of excluded records with reasons
  
  ...auditColumns,
}, (table) => ({
  processingRunIdx: uniqueIndex('datasets_processing_run_idx').on(table.processingRunId),
  projectIdx: index('datasets_project_idx').on(table.projectId),
}));

// ============================================================================
// AUDIT LOGS (PII handling compliance)
// ============================================================================

export const auditLogs = pgTable('audit_logs', {
  id: serial('id').primaryKey(),
  organizationId: integer('organization_id')
    .notNull()
    .references(() => organizations.id, { onDelete: 'cascade' }),
  userId: integer('user_id')
    .references(() => users.id, { onDelete: 'set null' }), // Preserve logs if user deleted
  
  // Action details
  action: text('action').notNull(),
  // 'pii_detected' | 'pii_masked' | 'data_exported' | 'source_synced' | 'config_changed'
  
  resourceType: text('resource_type').notNull(),
  // 'source' | 'processing_run' | 'dataset'
  
  resourceId: integer('resource_id').notNull(),
  
  // Details
  details: jsonb('details'), // Action-specific metadata
  
  // IP and session info
  ipAddress: text('ip_address'),
  userAgent: text('user_agent'),
  
  createdAt: timestamp('created_at').defaultNow().notNull(),
}, (table) => ({
  organizationIdx: index('audit_logs_organization_idx').on(table.organizationId),
  userIdx: index('audit_logs_user_idx').on(table.userId),
  actionIdx: index('audit_logs_action_idx').on(table.action),
  createdAtIdx: index('audit_logs_created_at_idx').on(table.createdAt),
  resourceIdx: index('audit_logs_resource_idx').on(table.resourceType, table.resourceId),
}));

// ============================================================================
// SQL HELPER FOR PARTIAL INDEX
// ============================================================================

import { sql } from 'drizzle-orm';
```

---

## 3. Relationships

### Foreign Key Definitions

| Table | Foreign Key | References | On Delete |
|-------|-------------|------------|-----------|
| users | organization_id | organizations.id | CASCADE |
| invitations | organization_id | organizations.id | CASCADE |
| invitations | invited_by_id | users.id | CASCADE |
| password_resets | user_id | users.id | CASCADE |
| projects | organization_id | organizations.id | CASCADE |
| sources | project_id | projects.id | CASCADE |
| schema_mappings | source_id | sources.id | CASCADE |
| deidentification_configs | source_id | sources.id | CASCADE |
| processing_configs | project_id | projects.id | CASCADE |
| processing_runs | project_id | projects.id | CASCADE |
| datasets | processing_run_id | processing_runs.id | CASCADE |
| datasets | project_id | projects.id | CASCADE |
| audit_logs | organization_id | organizations.id | CASCADE |
| audit_logs | user_id | users.id | SET NULL |

### Cascade Behavior Rationale

| Relationship | Behavior | Rationale |
|--------------|----------|-----------|
| Organization → Users | CASCADE | Users can't exist without organization |
| Organization → Projects | CASCADE | Projects belong to organization |
| User → Invitations | CASCADE | Invitations tied to inviter |
| User → Password Resets | CASCADE | Tokens belong to user |
| Project → Sources | CASCADE | Sources belong to project |
| Source → Schema Mapping | CASCADE | 1:1, config tied to source |
| Source → Deidentification Config | CASCADE | 1:1, config tied to source |
| Project → Processing Config | CASCADE | 1:1, config tied to project |
| Project → Processing Runs | CASCADE | Runs belong to project |
| Processing Run → Dataset | CASCADE | 1:1, output tied to run |
| User → Audit Logs | SET NULL | Preserve audit trail, show "deleted user" |

### Cardinality Summary

| Relationship | Cardinality |
|--------------|-------------|
| Organization ↔ User | 1:N |
| Organization ↔ Project | 1:N |
| Organization ↔ Invitation | 1:N |
| User ↔ Invitation (inviter) | 1:N |
| User ↔ Password Reset | 1:N |
| Project ↔ Source | 1:N |
| Source ↔ Schema Mapping | 1:1 |
| Source ↔ Deidentification Config | 1:1 |
| Project ↔ Processing Config | 1:1 |
| Project ↔ Processing Run | 1:N |
| Processing Run ↔ Dataset | 1:1 |
| Organization ↔ Audit Log | 1:N |

---

## 4. Index Strategy

### Index Definitions

| Table | Index Name | Columns | Type | Rationale |
|-------|------------|---------|------|-----------|
| users | users_organization_idx | organization_id | B-tree | Multi-tenant queries |
| users | users_email_idx | email | Unique | Login lookups |
| invitations | invitations_organization_idx | organization_id | B-tree | List pending invitations |
| invitations | invitations_token_idx | token | Unique | Token validation |
| invitations | invitations_email_org_idx | email, organization_id | B-tree | Check duplicate invites |
| password_resets | password_resets_user_idx | user_id | B-tree | User's active tokens |
| projects | projects_organization_idx | organization_id | B-tree | Multi-tenant queries |
| projects | projects_name_org_idx | organization_id, name | Unique (partial) | Unique names per org |
| sources | sources_project_idx | project_id | B-tree | List project sources |
| sources | sources_status_idx | status | B-tree | Filter by status |
| schema_mappings | schema_mappings_source_idx | source_id | Unique | 1:1 lookup |
| deidentification_configs | deidentification_configs_source_idx | source_id | Unique | 1:1 lookup |
| processing_configs | processing_configs_project_idx | project_id | Unique | 1:1 lookup |
| processing_runs | processing_runs_project_idx | project_id | B-tree | List project runs |
| processing_runs | processing_runs_status_idx | status | B-tree | Job queue polling |
| processing_runs | processing_runs_created_at_idx | created_at | B-tree | History sorting |
| datasets | datasets_processing_run_idx | processing_run_id | Unique | 1:1 lookup |
| datasets | datasets_project_idx | project_id | B-tree | List project datasets |
| audit_logs | audit_logs_organization_idx | organization_id | B-tree | Compliance queries |
| audit_logs | audit_logs_user_idx | user_id | B-tree | User activity |
| audit_logs | audit_logs_action_idx | action | B-tree | Action filtering |
| audit_logs | audit_logs_created_at_idx | created_at | B-tree | Time-range queries |
| audit_logs | audit_logs_resource_idx | resource_type, resource_id | B-tree | Resource lookup |

### Composite Index Notes

The `processing_runs_status_idx` is critical for the PostgreSQL-based job queue (per Architecture ADR-007). The worker polls for `status = 'pending'` records every 5 seconds.

---

## 5. Query Patterns

### Common Query Examples (Core Select API Only)

```typescript
import { db } from './db';
import { 
  users, organizations, projects, sources, 
  schemaMappings, processingRuns, datasets 
} from './db/schema';
import { eq, and, isNull, desc, count, sql } from 'drizzle-orm';

// ============================================================================
// AUTHENTICATION QUERIES
// ============================================================================

// Find user by email for login
async function findUserByEmail(email: string) {
  const [user] = await db
    .select()
    .from(users)
    .where(eq(users.email, email))
    .limit(1);
  return user;
}

// Find user with organization (for JWT payload)
async function findUserWithOrg(userId: number) {
  const [result] = await db
    .select({
      user: users,
      organization: organizations,
    })
    .from(users)
    .innerJoin(organizations, eq(users.organizationId, organizations.id))
    .where(eq(users.id, userId))
    .limit(1);
  return result;
}

// ============================================================================
// MULTI-TENANT QUERIES (Always scope by organizationId)
// ============================================================================

// List projects for organization (excluding soft-deleted)
async function findProjectsByOrg(organizationId: number) {
  return db
    .select()
    .from(projects)
    .where(
      and(
        eq(projects.organizationId, organizationId),
        isNull(projects.deletedAt)
      )
    )
    .orderBy(desc(projects.updatedAt));
}

// List users in organization
async function findUsersByOrg(organizationId: number) {
  return db
    .select({
      id: users.id,
      email: users.email,
      name: users.name,
      role: users.role,
      createdAt: users.createdAt,
    })
    .from(users)
    .where(eq(users.organizationId, organizationId))
    .orderBy(users.name);
}

// ============================================================================
// PROJECT QUERIES WITH AGGREGATIONS (Avoiding N+1)
// ============================================================================

// List projects with source and dataset counts
async function findProjectsWithCounts(organizationId: number) {
  const projectList = await db
    .select({
      project: projects,
      sourceCount: count(sources.id),
    })
    .from(projects)
    .leftJoin(sources, eq(sources.projectId, projects.id))
    .where(
      and(
        eq(projects.organizationId, organizationId),
        isNull(projects.deletedAt)
      )
    )
    .groupBy(projects.id)
    .orderBy(desc(projects.updatedAt));

  return projectList;
}

// ============================================================================
// SOURCE QUERIES WITH RELATED CONFIGS
// ============================================================================

// Find source with schema mapping and deidentification config
async function findSourceWithConfigs(sourceId: number, organizationId: number) {
  const [result] = await db
    .select({
      source: sources,
      schemaMapping: schemaMappings,
      deidentificationConfig: deidentificationConfigs,
    })
    .from(sources)
    .innerJoin(projects, eq(sources.projectId, projects.id))
    .leftJoin(schemaMappings, eq(schemaMappings.sourceId, sources.id))
    .leftJoin(deidentificationConfigs, eq(deidentificationConfigs.sourceId, sources.id))
    .where(
      and(
        eq(sources.id, sourceId),
        eq(projects.organizationId, organizationId)
      )
    )
    .limit(1);
  return result;
}

// ============================================================================
// JOB QUEUE QUERIES (Per Architecture ADR-007)
// ============================================================================

// Get next pending job for processing
async function getNextPendingJob() {
  const [job] = await db
    .select()
    .from(processingRuns)
    .where(eq(processingRuns.status, 'pending'))
    .orderBy(processingRuns.createdAt)
    .limit(1);
  return job;
}

// Update job progress
async function updateJobProgress(runId: number, progress: number, stage: string) {
  await db
    .update(processingRuns)
    .set({
      progress,
      currentStage: stage,
      updatedAt: new Date(),
    })
    .where(eq(processingRuns.id, runId));
}

// Check if job was cancelled
async function isJobCancelled(runId: number): Promise<boolean> {
  const [job] = await db
    .select({ status: processingRuns.status })
    .from(processingRuns)
    .where(eq(processingRuns.id, runId))
    .limit(1);
  return job?.status === 'cancelled';
}

// ============================================================================
// TRANSACTION EXAMPLE
// ============================================================================

// Create project with initial processing config
async function createProjectWithConfig(
  organizationId: number,
  data: { name: string; description?: string }
) {
  return db.transaction(async (tx) => {
    const [project] = await tx
      .insert(projects)
      .values({
        organizationId,
        name: data.name,
        description: data.description,
      })
      .returning();

    await tx
      .insert(processingConfigs)
      .values({
        projectId: project.id,
        stages: [],
        filters: {},
        roleIdentification: {},
        exportFormat: 'jsonl',
      });

    return project;
  });
}

// ============================================================================
// SOFT DELETE PATTERN
// ============================================================================

// Soft delete project
async function softDeleteProject(projectId: number, organizationId: number) {
  await db
    .update(projects)
    .set({ deletedAt: new Date(), updatedAt: new Date() })
    .where(
      and(
        eq(projects.id, projectId),
        eq(projects.organizationId, organizationId)
      )
    );
}

// Restore soft-deleted project
async function restoreProject(projectId: number, organizationId: number) {
  await db
    .update(projects)
    .set({ deletedAt: null, updatedAt: new Date() })
    .where(
      and(
        eq(projects.id, projectId),
        eq(projects.organizationId, organizationId)
      )
    );
}
```

---

## 6. Migration Strategy

### Drizzle Configuration

```typescript
// drizzle.config.ts
import { defineConfig } from 'drizzle-kit';

export default defineConfig({
  schema: './server/db/schema.ts',
  out: './drizzle',
  dialect: 'postgresql',
  dbCredentials: {
    url: process.env.DATABASE_URL!,
  },
});
```

### Migration Scripts

```json
// package.json (partial)
{
  "scripts": {
    "db:generate": "drizzle-kit generate",
    "db:migrate": "tsx server/db/migrate.ts",
    "db:push": "drizzle-kit push",
    "db:seed": "tsx server/db/seed.ts",
    "db:studio": "drizzle-kit studio"
  }
}
```

```typescript
// server/db/migrate.ts
import postgres from 'postgres';
import { drizzle } from 'drizzle-orm/postgres-js';
import { migrate } from 'drizzle-orm/postgres-js/migrator';

const sql = postgres(process.env.DATABASE_URL!, { max: 1 });
const db = drizzle(sql);

async function runMigrations() {
  console.log('Running migrations...');
  await migrate(db, { migrationsFolder: './drizzle' });
  console.log('Migrations complete');
  await sql.end();
  process.exit(0);
}

runMigrations().catch((err) => {
  console.error('Migration failed:', err);
  process.exit(1);
});
```

### Database Connection

```typescript
// server/db/index.ts
import postgres from 'postgres';
import { drizzle } from 'drizzle-orm/postgres-js';
import * as schema from './schema';

const connectionString = process.env.DATABASE_URL;

if (!connectionString) {
  throw new Error('DATABASE_URL environment variable is required');
}

const sql = postgres(connectionString, {
  max: 10,              // Connection pool size
  idle_timeout: 20,     // Close idle connections after 20s
  connect_timeout: 10,  // Connection timeout
});

export const db = drizzle(sql, { schema });

export async function closeDatabase() {
  await sql.end();
}
```

### Seed Data

```typescript
// server/db/seed.ts
import { db } from './index';
import { 
  organizations, users, projects, sources, 
  schemaMappings, deidentificationConfigs, processingConfigs 
} from './schema';
import bcrypt from 'bcryptjs';

async function seed() {
  console.log('Seeding database...');

  // Create test organization
  const [org] = await db
    .insert(organizations)
    .values({ name: 'Acme Corp' })
    .returning();

  console.log(`Created organization: ${org.name} (ID: ${org.id})`);

  // Create admin user
  const [adminUser] = await db
    .insert(users)
    .values({
      organizationId: org.id,
      email: 'admin@example.com',
      passwordHash: await bcrypt.hash('Admin123!', 10),
      name: 'Admin User',
      role: 'admin',
    })
    .returning();

  console.log(`Created admin user: ${adminUser.email}`);

  // Create member user
  const [memberUser] = await db
    .insert(users)
    .values({
      organizationId: org.id,
      email: 'member@example.com',
      passwordHash: await bcrypt.hash('Member123!', 10),
      name: 'Team Member',
      role: 'member',
    })
    .returning();

  console.log(`Created member user: ${memberUser.email}`);

  // Create sample project
  const [project] = await db
    .insert(projects)
    .values({
      organizationId: org.id,
      name: 'Support Ticket Analysis',
      description: 'Preparing support tickets for AI agent training',
    })
    .returning();

  console.log(`Created project: ${project.name}`);

  // Create processing config for project
  await db
    .insert(processingConfigs)
    .values({
      projectId: project.id,
      stages: [
        { name: 'ingest', enabled: true, config: {} },
        { name: 'normalize', enabled: true, config: {} },
        { name: 'deidentify', enabled: true, config: {} },
        { name: 'filter', enabled: true, config: {} },
        { name: 'export', enabled: true, config: {} },
      ],
      filters: {
        minMessageLength: 10,
        requireCompleteConversation: true,
      },
      exportFormat: 'jsonl',
      isConfigured: true,
    });

  // Create sample file source
  const [source] = await db
    .insert(sources)
    .values({
      projectId: project.id,
      name: 'January Tickets Export',
      type: 'file',
      status: 'ready',
      fileName: 'tickets-jan-2026.csv',
      fileType: 'csv',
      detectedFields: [
        { name: 'ticket_id', type: 'string', samples: ['T-001', 'T-002'] },
        { name: 'customer_email', type: 'string', samples: ['john@example.com'] },
        { name: 'message', type: 'string', samples: ['Hello, I need help...'] },
        { name: 'agent_response', type: 'string', samples: ['Hi John, I can help...'] },
      ],
    })
    .returning();

  console.log(`Created source: ${source.name}`);

  // Create schema mapping for source
  await db
    .insert(schemaMappings)
    .values({
      sourceId: source.id,
      mappings: [
        { sourceField: 'ticket_id', targetField: 'conversation_id', confidence: 'high' },
        { sourceField: 'customer_email', targetField: 'user_id', confidence: 'medium' },
        { sourceField: 'message', targetField: 'user_message', confidence: 'high' },
        { sourceField: 'agent_response', targetField: 'assistant_message', confidence: 'high' },
      ],
      targetSchema: 'conversation',
      isConfigured: true,
    });

  // Create deidentification config for source
  await db
    .insert(deidentificationConfigs)
    .values({
      sourceId: source.id,
      enabledTypes: ['email', 'phone', 'name'],
      customPatterns: [],
      maskingStrategy: 'replacement',
      isConfigured: true,
    });

  console.log('Seed complete!');
  console.log('\nTest Credentials:');
  console.log('  Admin: admin@example.com / Admin123!');
  console.log('  Member: member@example.com / Member123!');

  process.exit(0);
}

seed().catch((err) => {
  console.error('Seed failed:', err);
  process.exit(1);
});
```

### Rollback Procedures

1. **Generate rollback migrations**: Drizzle Kit generates down migrations in the `/drizzle` folder
2. **Manual rollback**: For emergency rollback, use direct SQL:
   ```sql
   -- Example: Rollback soft delete column addition
   ALTER TABLE projects DROP COLUMN deleted_at;
   ```
3. **Point-in-time recovery**: Rely on Neon's branching for database state recovery

---

## 7. Type Exports

```typescript
// server/db/schema.ts (type exports section)

// Organization types
export type Organization = typeof organizations.$inferSelect;
export type NewOrganization = typeof organizations.$inferInsert;

// User types
export type User = typeof users.$inferSelect;
export type NewUser = typeof users.$inferInsert;

// Invitation types
export type Invitation = typeof invitations.$inferSelect;
export type NewInvitation = typeof invitations.$inferInsert;

// Password reset types
export type PasswordReset = typeof passwordResets.$inferSelect;
export type NewPasswordReset = typeof passwordResets.$inferInsert;

// Project types
export type Project = typeof projects.$inferSelect;
export type NewProject = typeof projects.$inferInsert;

// Source types
export type Source = typeof sources.$inferSelect;
export type NewSource = typeof sources.$inferInsert;

// Schema mapping types
export type SchemaMapping = typeof schemaMappings.$inferSelect;
export type NewSchemaMapping = typeof schemaMappings.$inferInsert;

// Deidentification config types
export type DeidentificationConfig = typeof deidentificationConfigs.$inferSelect;
export type NewDeidentificationConfig = typeof deidentificationConfigs.$inferInsert;

// Processing config types
export type ProcessingConfig = typeof processingConfigs.$inferSelect;
export type NewProcessingConfig = typeof processingConfigs.$inferInsert;

// Processing run types
export type ProcessingRun = typeof processingRuns.$inferSelect;
export type NewProcessingRun = typeof processingRuns.$inferInsert;

// Dataset types
export type Dataset = typeof datasets.$inferSelect;
export type NewDataset = typeof datasets.$inferInsert;

// Audit log types
export type AuditLog = typeof auditLogs.$inferSelect;
export type NewAuditLog = typeof auditLogs.$inferInsert;

// ============================================================================
// JSONB TYPE DEFINITIONS (for type safety)
// ============================================================================

export interface FieldMapping {
  sourceField: string;
  targetField: string | null;
  confidence: 'high' | 'medium' | 'low' | 'manual';
  transform?: string;
}

export interface DetectedField {
  name: string;
  type: string;
  samples: string[];
}

export interface CustomPIIPattern {
  name: string;
  pattern: string;
  replacement: string;
}

export interface PipelineStage {
  name: 'ingest' | 'normalize' | 'deidentify' | 'filter' | 'role_identify' | 'export';
  enabled: boolean;
  config: Record<string, unknown>;
}

export interface QualityFilters {
  minMessageLength?: number;
  requireCompleteConversation?: boolean;
  excludePatterns?: string[];
}

export interface RoleIdentificationConfig {
  enabled: boolean;
  agentPatterns?: string[];
  customerPatterns?: string[];
  systemPatterns?: string[];
}

export type PIIType = 'email' | 'phone' | 'name' | 'address' | 'ssn' | 'credit_card' | 'dob';
export type MaskingStrategy = 'replacement' | 'redaction' | 'pseudonymization';
export type SourceType = 'file' | 'teamwork_desk';
export type SourceStatus = 'pending' | 'connected' | 'syncing' | 'ready' | 'error';
export type RunStatus = 'pending' | 'processing' | 'completed' | 'cancelled' | 'failed';
export type ExportFormat = 'jsonl' | 'json' | 'csv' | 'qa_pairs';
export type TargetSchema = 'conversation' | 'qa_pairs' | 'raw';
export type UserRole = 'admin' | 'member';
```

---

## 8. Document Validation

### CHECKPOINT 3: Final Validation

### Completeness Checklist

- [x] All PRD entities represented
- [x] All relationships defined with cascade behavior
- [x] Foreign keys indexed
- [x] Audit columns on all tables
- [x] Migration scripts specified
- [x] Type exports defined
- [x] Query patterns documented (Core Select API only)
- [x] Soft delete pattern implemented (projects)
- [x] Multi-tenant scoping documented
- [x] Job queue pattern implemented (processing_runs)

### Prompt Hygiene Gate (Constitution Section L)

- [x] Framework Version header present and correct
- [x] Encoding scan: No non-ASCII artifact tokens
- [x] Inheritance references Constitution v3.3
- [x] No full global rule restatements (uses "Per Constitution Section X")

### Entity Coverage

| PRD Entity | Schema Table | Status |
|------------|--------------|--------|
| User | users | ✅ Complete |
| Organization | organizations | ✅ Complete |
| Invitation | invitations | ✅ Complete |
| PasswordReset | password_resets | ✅ Complete |
| Project | projects | ✅ Complete |
| Source | sources | ✅ Complete |
| SchemaMapping | schema_mappings | ✅ Complete |
| DeidentificationConfig | deidentification_configs | ✅ Complete |
| ProcessingConfig | processing_configs | ✅ Complete |
| ProcessingRun | processing_runs | ✅ Complete |
| Dataset | datasets | ✅ Complete |
| AuditLog | audit_logs | ✅ Complete |

### Architecture Compatibility

| Requirement | Status | Notes |
|-------------|--------|-------|
| postgres-js driver | ✅ | Per Constitution Section D |
| Drizzle ORM | ✅ | Core Select API only |
| Connection pooling | ✅ | max: 10, idle_timeout: 20 |
| Encrypted API keys | ✅ | TEXT column, iv:encrypted:authTag format |
| Multi-tenant isolation | ✅ | organization_id on all relevant tables |
| Job queue pattern | ✅ | processing_runs with status polling |

### Document Status: COMPLETE

---

## 9. Downstream Agent Handoff Brief

### Agent 4: API Contract

**Entity Operations (CRUD per entity):**

| Entity | Create | Read | Update | Delete | List |
|--------|--------|------|--------|--------|------|
| organizations | - | ✅ | ✅ (admin) | - | - |
| users | ✅ (via invite) | ✅ (self) | ✅ (self) | - | ✅ (org members) |
| invitations | ✅ (admin) | ✅ | - | ✅ | ✅ (pending) |
| projects | ✅ | ✅ | ✅ | ✅ (soft) | ✅ |
| sources | ✅ | ✅ | ✅ | ✅ | ✅ |
| schema_mappings | - | ✅ | ✅ | - | - |
| deidentification_configs | - | ✅ | ✅ | - | - |
| processing_configs | - | ✅ | ✅ | - | - |
| processing_runs | ✅ | ✅ | ✅ (cancel) | - | ✅ |
| datasets | - | ✅ | - | - | ✅ |

**Relationship Traversal Patterns:**
- Project → Sources (list sources for project)
- Source → SchemaMapping + DeidentificationConfig (get with configs)
- Project → ProcessingRuns (list run history)
- ProcessingRun → Dataset (get output)

**Pagination Requirements:**
- projects list: page, pageSize, default 20
- sources list: page, pageSize, default 20
- processing_runs list: page, pageSize, default 10
- datasets list: page, pageSize, default 10
- audit_logs list: page, pageSize, default 50

### Agent 5: UI/UX Specification

**Data Shapes for Forms:**
- Project: name (required), description (optional)
- Source (file): file upload, name (auto from filename)
- Source (API): apiKey, apiSubdomain, dataTypes[]
- SchemaMapping: mappings array with dropdowns
- DeidentificationConfig: enabledTypes checkboxes, maskingStrategy radio
- ProcessingConfig: stages toggles, filters inputs

**List/Detail Patterns:**
- Projects: Card grid with name, description, source count, last updated
- Sources: Table with name, type, status, last sync
- Processing Runs: Table with date, status, progress bar, record counts
- Datasets: Table with name, format, record count, download button

**Relationship Displays:**
- Source detail shows schema mapping and deidentification inline
- Project detail shows sources list and processing runs tabs

### Agent 6: Implementation Orchestrator

**File Structure:**
```
server/
  db/
    index.ts      # Connection + db export
    schema.ts     # All table definitions + types
    migrate.ts    # Migration runner
    seed.ts       # Development seed data
```

**Commands:**
- `npm run db:generate` - Generate migrations from schema changes
- `npm run db:migrate` - Run pending migrations
- `npm run db:push` - Push schema directly (dev only)
- `npm run db:seed` - Seed development data

**Type Imports:**
```typescript
import { db } from '@/server/db';
import { 
  users, projects, sources,
  type User, type Project, type Source 
} from '@/server/db/schema';
```

### Agent 7: QA & Deployment

**Seed Data for Testing:**
- Organization: Acme Corp
- Admin user: admin@example.com / Admin123!
- Member user: member@example.com / Member123!
- Sample project with file source

**Migration Verification Steps:**
1. Run `npm run db:generate` - Verify no errors
2. Run `npm run db:migrate` - Verify all migrations apply
3. Run `npm run db:seed` - Verify seed data inserts
4. Query each table to verify schema

**Database Health Checks:**
- Connection pool active connections
- Migration version current
- No orphaned records (cascade verification)

---

## ASSUMPTION REGISTER

### AR-001: Single Organization per User

- **Type:** ASSUMPTION (inherited from PRD AR-001)
- **Source Gap:** PRD assumes users belong to exactly one organization
- **Assumption Made:** No junction table for user-organization many-to-many
- **Impact if Wrong:** Need `user_organizations` junction table, auth context changes, org switcher UI
- **Proposed Resolution:** Confirm requirement before production
- **Status:** UNRESOLVED (inherited)
- **Owner:** Human
- **Date:** 2026-01-22

### AR-002: JSONB for Configuration Storage

- **Type:** ASSUMPTION
- **Source Gap:** PRD doesn't specify exact structure of mapping/config data
- **Assumption Made:** JSONB columns for flexible schema (mappings, stages, filters)
- **Impact if Wrong:** May need normalized tables if query performance suffers
- **Proposed Resolution:** Monitor query performance; migrate to normalized tables if needed
- **Status:** ACCEPTED
- **Owner:** Agent 3
- **Date:** 2026-01-22

### AR-003: Dataset Data Storage

- **Type:** ASSUMPTION
- **Source Gap:** Architecture doesn't specify where processed data is stored
- **Assumption Made:** JSONB column for datasets up to reasonable size; file export for download
- **Impact if Wrong:** May need file-based storage for large datasets (>10MB)
- **Proposed Resolution:** Add file_path column for large dataset export; JSONB for preview
- **Status:** ACCEPTED
- **Owner:** Agent 3
- **Date:** 2026-01-22

### AR-004: Audit Log Retention

- **Type:** ASSUMPTION
- **Source Gap:** PRD mentions audit logging but Architecture specifies 90-day retention
- **Assumption Made:** No automatic purge mechanism in schema; handled by scheduled job
- **Impact if Wrong:** Table bloat if not purged
- **Proposed Resolution:** Implement scheduled job to delete audit_logs older than 90 days
- **Status:** ACCEPTED
- **Owner:** Agent 6
- **Date:** 2026-01-22

### AR-005: Password Reset Token Storage

- **Type:** ASSUMPTION
- **Source Gap:** Architecture mentions token hash but doesn't specify algorithm
- **Assumption Made:** Store bcrypt hash of token (not plaintext)
- **Impact if Wrong:** Minimal; hash algorithm can be changed
- **Proposed Resolution:** Document hash algorithm in implementation
- **Status:** ACCEPTED
- **Owner:** Agent 3
- **Date:** 2026-01-22

---

## Document End
